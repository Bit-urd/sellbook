#!/usr/bin/env python3
"""
Â¢ûÈáèÂºèÂ§öÂ∫óÈì∫Áà¨Ëô´ - Êï∞ÊçÆÂ∫ìÁâàÊú¨
- ÊîØÊåÅÊñ≠ÁÇπÁª≠Áà¨ÂíåÂéªÈáç
- ÊØèÈ°µÊï∞ÊçÆÁ´ãÂç≥‰øùÂ≠òÂà∞SQLiteÊï∞ÊçÆÂ∫ì
- ÂêØÂä®Êó∂‰ªéÊï∞ÊçÆÂ∫ìÂä†ËΩΩÂ∑≤ÊúâÊï∞ÊçÆÔºåËá™Âä®ÂéªÈáç
"""
import asyncio
import os
import time
from datetime import datetime
from playwright.async_api import async_playwright
import aiohttp

from database import DatabaseManager, BookRepository

class IncrementalScraperV2:
    def __init__(self, shop_list_file="shop_list.txt"):
        self.shop_list_file = shop_list_file
        self.browser = None
        self.page = None
        self.playwright = None
        
        # Êï∞ÊçÆÂ∫ìÊìç‰Ωú
        self.book_repo = BookRepository()
        
        # Â∑≤Áà¨ÂèñÁöÑ‰π¶Á±çÈõÜÂêà(Áî®‰∫éÂø´ÈÄüÂéªÈáç)
        self.scraped_itemids = set()
        self.scraped_count = 0
        
        # ÁªüËÆ°‰ø°ÊÅØ
        self.stats = {
            'existing_records': 0,
            'new_records': 0,
            'duplicate_skipped': 0,
            'shops_processed': 0,
            'pages_processed': 0
        }

    async def load_existing_data(self):
        """‰ªéÊï∞ÊçÆÂ∫ìÂä†ËΩΩÂ∑≤ÊúâÊï∞ÊçÆÔºåÊûÑÂª∫ÂéªÈáçÈõÜÂêà"""
        print("üìÇ Ê£ÄÊü•Êï∞ÊçÆÂ∫ì‰∏≠ÁöÑÂ∑≤ÊúâÊï∞ÊçÆ...")
        
        try:
            # ÂàùÂßãÂåñÊï∞ÊçÆÂ∫ì
            db_manager = DatabaseManager()
            await db_manager.init_database()
            
            # Ëé∑ÂèñÂ∑≤Â≠òÂú®ÁöÑitemid
            self.scraped_itemids = await self.book_repo.get_existing_itemids()
            self.scraped_count = len(self.scraped_itemids)
            self.stats['existing_records'] = self.scraped_count
            
            if self.scraped_count > 0:
                print(f"üìä ‰ªéÊï∞ÊçÆÂ∫ìÂä†ËΩΩÂ∑≤ÊúâÊï∞ÊçÆ: {self.scraped_count} Êù°ËÆ∞ÂΩï")
                print(f"üîç ÂéªÈáçÈõÜÂêàÂ§ßÂ∞è: {len(self.scraped_itemids)} ‰∏™ItemID")
            else:
                print("üìù Êï∞ÊçÆÂ∫ì‰∏∫Á©∫ÔºåÂ∞ÜÂºÄÂßãÂÖ®Êñ∞Áà¨Âèñ")
                
        except Exception as e:
            print(f"‚ùå Âä†ËΩΩÊï∞ÊçÆÂ∫ìÊï∞ÊçÆÂ§±Ë¥•: {e}")
            self.scraped_itemids = set()

    def load_shop_list(self):
        """Âä†ËΩΩÂ∫óÈì∫IDÂàóË°®"""
        shops = []
        
        if not os.path.exists(self.shop_list_file):
            print(f"‚ùå Â∫óÈì∫ÂàóË°®Êñá‰ª∂‰∏çÂ≠òÂú®: {self.shop_list_file}")
            return shops
        
        with open(self.shop_list_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    try:
                        shop_id = line.strip()
                        shops.append(shop_id)
                    except ValueError:
                        continue
        
        print(f"üìã Âä†ËΩΩ‰∫Ü {len(shops)} ‰∏™Â∫óÈì∫ID")
        return shops

    async def connect_to_chrome(self):
        """ËøûÊé•Âà∞Áé∞ÊúâÁöÑChromeË∞ÉËØï‰ºöËØù"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get("http://localhost:9222/json/version") as response:
                    if response.status == 200:
                        version_info = await response.json()
                        ws_url = version_info.get('webSocketDebuggerUrl', '')
                        print("‚úÖ ChromeË∞ÉËØïÁ´ØÂè£ËøûÊé•ÊàêÂäü")
                    else:
                        print("‚ùå ChromeË∞ÉËØïÁ´ØÂè£Êó†ÂìçÂ∫î")
                        return False
        except Exception as e:
            print(f"‚ùå ËøûÊé•ChromeË∞ÉËØïÁ´ØÂè£Â§±Ë¥•: {e}")
            return False
        
        try:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.connect_over_cdp(ws_url)
            
            # ‰ΩøÁî®Áé∞ÊúâÊµèËßàÂô®‰∏ä‰∏ãÊñá
            contexts = self.browser.contexts
            if contexts:
                context = contexts[0]
                print("üîÑ ‰ΩøÁî®Áé∞ÊúâÊµèËßàÂô®‰∏ä‰∏ãÊñá")
            else:
                context = await self.browser.new_context()
                print("üÜï ÂàõÂª∫Êñ∞ÁöÑÊµèËßàÂô®‰∏ä‰∏ãÊñá")
            
            # ‰ΩøÁî®Áé∞ÊúâÈ°µÈù¢ÊàñÂàõÂª∫Êñ∞È°µÈù¢
            pages = context.pages
            if pages:
                self.page = pages[0]
                print("üîÑ ‰ΩøÁî®Áé∞ÊúâÈ°µÈù¢")
            else:
                self.page = await context.new_page()
                print("üÜï ÂàõÂª∫Êñ∞È°µÈù¢")
            
            return True
            
        except Exception as e:
            print(f"‚ùå PlaywrightËøûÊé•Â§±Ë¥•: {e}")
            return False

    async def extract_page_data(self, shop_id: str, page_num: int):
        """ÊèêÂèñÂΩìÂâçÈ°µÈù¢ÁöÑ‰π¶Á±çÊï∞ÊçÆ"""
        try:
            books = await self.page.evaluate("""
                () => {
                    const books = [];
                    const bookItems = document.querySelectorAll('.item-info, .shopLineBookItem');
                    
                    bookItems.forEach(item => {
                        try {
                            const book = {};
                            
                            // ‰π¶Á±çÈìæÊé•Âíåitemid
                            const linkElement = item.querySelector('a[href*="book.kongfz.com"]');
                            if (linkElement) {
                                book.book_url = linkElement.href;
                                const urlMatch = book.book_url.match(/\\/(\\d+)\\//);
                                if (urlMatch) {
                                    book.itemid = urlMatch[1];
                                }
                            }
                            
                            // ‰π¶Âêç
                            const titleElement = item.querySelector('.title, .title-link, h3 a, .bookTitle');
                            if (titleElement) {
                                book.title = titleElement.textContent.trim();
                            }
                            
                            // ‰ΩúËÄÖ
                            const authorElement = item.querySelector('.author, .bookAuthor');
                            if (authorElement) {
                                book.author = authorElement.textContent.trim();
                            }
                            
                            // Âá∫ÁâàÁ§æÂíåÂá∫ÁâàÂπ¥‰ªΩ
                            const publishElement = item.querySelector('.publish, .bookPub, .publisher');
                            if (publishElement) {
                                const publishText = publishElement.textContent.trim();
                                book.publisher = publishText.split(' ')[0] || '';
                                const yearMatch = publishText.match(/(\\d{4})/);
                                if (yearMatch) {
                                    book.publish_year = yearMatch[1];
                                }
                            }
                            
                            // ISBN
                            const isbnElement = item.querySelector('.isbn, [class*="isbn"]');
                            if (isbnElement) {
                                book.isbn = isbnElement.textContent.replace(/[^\\d]/g, '');
                            }
                            
                            // ‰ª∑Ê†º
                            const priceElement = item.querySelector('.price, .bookPrice, .itemPrice');
                            if (priceElement) {
                                const priceText = priceElement.textContent.trim();
                                const priceMatch = priceText.match(/(\\d+\\.?\\d*)/);
                                if (priceMatch) {
                                    book.price = priceMatch[1];
                                    book.display_price = priceText;
                                }
                            }
                            
                            // ÂìÅÁõ∏
                            const qualityElement = item.querySelector('.quality, .bookQuality');
                            if (qualityElement) {
                                book.quality = qualityElement.textContent.trim();
                            }
                            
                            // ÂÖ∂‰ªñ‰ø°ÊÅØ
                            const catnumElement = item.querySelector('.catnum');
                            if (catnumElement) {
                                book.catnum = catnumElement.textContent.trim();
                            }
                            
                            const useridElement = item.querySelector('[data-userid]');
                            if (useridElement) {
                                book.userid = useridElement.getAttribute('data-userid');
                            }
                            
                            if (book.itemid && book.title) {
                                books.push(book);
                            }
                            
                        } catch (error) {
                            console.log('ÊèêÂèñ‰π¶Á±ç‰ø°ÊÅØÊó∂Âá∫Èîô:', error);
                        }
                    });
                    
                    return books;
                }
            """)
            
            # Ë°•ÂÖÖÁà¨Âèñ‰ø°ÊÅØ
            current_time = datetime.now().isoformat()
            for book in books:
                book['shopid'] = shop_id
                book['scraped_time'] = current_time
                book['scraped_shop_id'] = shop_id
                book['scraped_page'] = page_num
            
            return books
            
        except Exception as e:
            print(f"‚ùå ÊèêÂèñÈ°µÈù¢Êï∞ÊçÆÂ§±Ë¥•: {e}")
            return []

    async def save_books_to_database(self, books_data):
        """‰øùÂ≠ò‰π¶Á±çÊï∞ÊçÆÂà∞Êï∞ÊçÆÂ∫ì"""
        if not books_data:
            return 0
        
        try:
            saved_count = await self.book_repo.save_books(books_data)
            return saved_count
        except Exception as e:
            print(f"‚ùå ‰øùÂ≠òÂà∞Êï∞ÊçÆÂ∫ìÂ§±Ë¥•: {e}")
            return 0

    async def scrape_shop(self, shop_id: str):
        """Áà¨ÂèñÂçï‰∏™Â∫óÈì∫ÁöÑÊâÄÊúâ‰π¶Á±ç"""
        print(f"\nüè™ ÂºÄÂßãÁà¨ÂèñÂ∫óÈì∫ {shop_id}")
        
        shop_url = f"https://shop{shop_id}.kongfz.com/book/"
        
        try:
            await self.page.goto(shop_url, wait_until='networkidle')
            await asyncio.sleep(3)
            
            page_num = 1
            max_pages = 1000  # ËÆæÁΩÆÊúÄÂ§ßÈ°µÊï∞ÈôêÂà∂
            consecutive_empty_pages = 0
            
            while page_num <= max_pages and consecutive_empty_pages < 3:
                print(f"üìö Ê≠£Âú®Áà¨ÂèñÁ¨¨ {page_num} È°µ...")
                
                # ÊèêÂèñÂΩìÂâçÈ°µÈù¢Êï∞ÊçÆ
                page_books = await self.extract_page_data(shop_id, page_num)
                
                if not page_books:
                    consecutive_empty_pages += 1
                    print(f"‚ö†Ô∏è  Á¨¨ {page_num} È°µÊó†Êï∞ÊçÆÔºåËøûÁª≠Á©∫È°µ: {consecutive_empty_pages}")
                    if consecutive_empty_pages >= 3:
                        print("üîö ËøûÁª≠3È°µÊó†Êï∞ÊçÆÔºåÁªìÊùüÂΩìÂâçÂ∫óÈì∫Áà¨Âèñ")
                        break
                else:
                    consecutive_empty_pages = 0
                
                # ÂéªÈáçÂ§ÑÁêÜ
                new_books = []
                duplicate_count = 0
                
                for book in page_books:
                    itemid = book.get('itemid', '')
                    if itemid and itemid not in self.scraped_itemids:
                        new_books.append(book)
                        self.scraped_itemids.add(itemid)
                    else:
                        duplicate_count += 1
                
                # ‰øùÂ≠òÊñ∞‰π¶Á±çÂà∞Êï∞ÊçÆÂ∫ì
                if new_books:
                    saved_count = await self.save_books_to_database(new_books)
                    self.stats['new_records'] += saved_count
                    print(f"üìö Á¨¨ {page_num} È°µ: {len(page_books)} Êú¨‰π¶ÔºåÊñ∞Â¢û {len(new_books)} Êú¨")
                    print(f"üíæ ÊàêÂäü‰øùÂ≠ò {saved_count} Êù°ËÆ∞ÂΩïÂà∞Êï∞ÊçÆÂ∫ì")
                else:
                    print(f"üìö Á¨¨ {page_num} È°µ: {len(page_books)} Êú¨‰π¶ÔºåÊñ∞Â¢û 0 Êú¨")
                
                if duplicate_count > 0:
                    self.stats['duplicate_skipped'] += duplicate_count
                    print(f"üîÑ Ë∑≥Ëøá {duplicate_count} Êù°ÈáçÂ§çËÆ∞ÂΩï")
                
                self.stats['pages_processed'] += 1
                
                # Â¶ÇÊûúÊï¥È°µÈÉΩÊòØÈáçÂ§çÊï∞ÊçÆÔºåÂèØËÉΩÂ∑≤ÁªèÁà¨ÂÆå‰∫Ü
                if duplicate_count == len(page_books) and len(page_books) > 0:
                    print("‚úÖ ÂΩìÂâçÈ°µÂÖ®ÈÉ®‰∏∫ÈáçÂ§çÊï∞ÊçÆÔºåÂèØËÉΩÂ∑≤Áà¨ÂèñÂÆåÊàê")
                    break
                
                # Â∞ùËØïÁøªÂà∞‰∏ã‰∏ÄÈ°µ
                if not await self.go_to_next_page():
                    print("üìÑ Êó†Ê≥ïÁøªÂà∞‰∏ã‰∏ÄÈ°µÔºåÁªìÊùüÂΩìÂâçÂ∫óÈì∫")
                    break
                
                page_num += 1
                await asyncio.sleep(2)  # È°µÈù¢Èó¥Èöî
            
            self.stats['shops_processed'] += 1
            print(f"‚úÖ Â∫óÈì∫ {shop_id} Áà¨ÂèñÂÆåÊàê")
            
        except Exception as e:
            print(f"‚ùå Áà¨ÂèñÂ∫óÈì∫ {shop_id} Â§±Ë¥•: {e}")

    async def go_to_next_page(self):
        """Â∞ùËØïÁøªÂà∞‰∏ã‰∏ÄÈ°µ"""
        try:
            # Â∏∏ËßÅÁöÑ‰∏ã‰∏ÄÈ°µÈÄâÊã©Âô®
            next_selectors = [
                'a.next:not(.disabled)',
                'a[title="‰∏ã‰∏ÄÈ°µ"]:not(.disabled)',
                '.pagination a.next',
                'a:has-text("‰∏ã‰∏ÄÈ°µ")',
                '.page-next:not(.disabled)'
            ]
            
            for selector in next_selectors:
                try:
                    next_btn = await self.page.query_selector(selector)
                    if next_btn:
                        await next_btn.click()
                        await self.page.wait_for_load_state('networkidle')
                        await asyncio.sleep(1)
                        return True
                except:
                    continue
            
            return False
            
        except Exception as e:
            print(f"ÁøªÈ°µÂ§±Ë¥•: {e}")
            return False

    def print_final_stats(self):
        """ÊâìÂç∞ÊúÄÁªàÁªüËÆ°‰ø°ÊÅØ"""
        print("\n" + "="*50)
        print("üìä Áà¨ÂèñÂÆåÊàêÁªüËÆ°")
        print("="*50)
        print(f"üè™ Â§ÑÁêÜÂ∫óÈì∫Êï∞: {self.stats['shops_processed']}")
        print(f"üìÑ Â§ÑÁêÜÈ°µÈù¢Êï∞: {self.stats['pages_processed']}")
        print(f"üìö ÂéüÊúâËÆ∞ÂΩïÊï∞: {self.stats['existing_records']}")
        print(f"üÜï Êñ∞Â¢ûËÆ∞ÂΩïÊï∞: {self.stats['new_records']}")
        print(f"üîÑ Ë∑≥ËøáÈáçÂ§çÊï∞: {self.stats['duplicate_skipped']}")
        print(f"üìä ÊÄªËÆ∞ÂΩïÊï∞: {self.stats['existing_records'] + self.stats['new_records']}")
        print("="*50)

    async def run(self):
        """ËøêË°åÂ¢ûÈáèÁà¨Ëô´"""
        print("üöÄ ÂêØÂä®Â¢ûÈáèÂºèÂ§öÂ∫óÈì∫Áà¨Ëô´ (Êï∞ÊçÆÂ∫ìÁâà)")
        
        # 1. Âä†ËΩΩÂ∑≤ÊúâÊï∞ÊçÆ
        await self.load_existing_data()
        
        # 2. Âä†ËΩΩÂ∫óÈì∫ÂàóË°®
        shop_list = self.load_shop_list()
        if not shop_list:
            print("‚ùå Ê≤°ÊúâÊúâÊïàÁöÑÂ∫óÈì∫IDÔºåÈÄÄÂá∫")
            return
        
        # 3. ËøûÊé•Âà∞Chrome
        if not await self.connect_to_chrome():
            print("‚ùå Êó†Ê≥ïËøûÊé•Âà∞ChromeÔºåËØ∑ÂÖàÂêØÂä®ChromeË∞ÉËØïÊ®°Âºè")
            return
        
        try:
            # 4. ÈÄê‰∏™Â∫óÈì∫Áà¨Âèñ
            for i, shop_id in enumerate(shop_list, 1):
                print(f"\n{'='*20} Â∫óÈì∫ {i}/{len(shop_list)} {'='*20}")
                await self.scrape_shop(shop_id)
                
                # Â∫óÈì∫Èó¥Á≠âÂæÖ
                if i < len(shop_list):
                    print("‚è≥ Â∫óÈì∫Èó¥Á≠âÂæÖ 5 Áßí...")
                    await asyncio.sleep(5)
            
            # 5. ÊâìÂç∞ÊúÄÁªàÁªüËÆ°
            self.print_final_stats()
            
        except KeyboardInterrupt:
            print("\n‚ö†Ô∏è  Áî®Êà∑‰∏≠Êñ≠Áà¨Âèñ")
            self.print_final_stats()
        except Exception as e:
            print(f"\n‚ùå Áà¨ÂèñËøáÁ®ãÂá∫Èîô: {e}")
            self.print_final_stats()
        finally:
            await self.cleanup()

    async def cleanup(self):
        """Ê∏ÖÁêÜËµÑÊ∫ê"""
        try:
            if self.playwright:
                await self.playwright.stop()
                print("üßπ ÊµèËßàÂô®ËøûÊé•Â∑≤ÂÖ≥Èó≠")
        except:
            pass

async def main():
    scraper = IncrementalScraperV2()
    await scraper.run()

if __name__ == "__main__":
    asyncio.run(main())