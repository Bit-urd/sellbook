#!/usr/bin/env python3
"""
å¢é‡å¼å¤šåº—é“ºçˆ¬è™« - æ•°æ®åº“ç‰ˆæœ¬
- æ”¯æŒæ–­ç‚¹ç»­çˆ¬å’Œå»é‡
- æ¯é¡µæ•°æ®ç«‹å³ä¿å­˜åˆ°SQLiteæ•°æ®åº“
- å¯åŠ¨æ—¶ä»æ•°æ®åº“åŠ è½½å·²æœ‰æ•°æ®ï¼Œè‡ªåŠ¨å»é‡
"""
import asyncio
import os
import time
from datetime import datetime
from playwright.async_api import async_playwright
import aiohttp

from database import DatabaseManager, BookRepository

class IncrementalScraperV2:
    def __init__(self, shop_list_file="shop_list.txt"):
        self.shop_list_file = shop_list_file
        self.browser = None
        self.page = None
        self.playwright = None
        
        # æ•°æ®åº“æ“ä½œ
        self.book_repo = BookRepository()
        
        # å·²çˆ¬å–çš„ä¹¦ç±é›†åˆ(ç”¨äºå¿«é€Ÿå»é‡)
        self.scraped_itemids = set()
        self.scraped_count = 0
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'existing_records': 0,
            'new_records': 0,
            'duplicate_skipped': 0,
            'shops_processed': 0,
            'pages_processed': 0
        }

    async def load_existing_data(self):
        """ä»æ•°æ®åº“åŠ è½½å·²æœ‰æ•°æ®ï¼Œæ„å»ºå»é‡é›†åˆ"""
        print("ğŸ“‚ æ£€æŸ¥æ•°æ®åº“ä¸­çš„å·²æœ‰æ•°æ®...")
        
        try:
            # åˆå§‹åŒ–æ•°æ®åº“
            db_manager = DatabaseManager()
            await db_manager.init_database()
            
            # è·å–å·²å­˜åœ¨çš„itemid
            self.scraped_itemids = await self.book_repo.get_existing_itemids()
            self.scraped_count = len(self.scraped_itemids)
            self.stats['existing_records'] = self.scraped_count
            
            if self.scraped_count > 0:
                print(f"ğŸ“Š ä»æ•°æ®åº“åŠ è½½å·²æœ‰æ•°æ®: {self.scraped_count} æ¡è®°å½•")
                print(f"ğŸ” å»é‡é›†åˆå¤§å°: {len(self.scraped_itemids)} ä¸ªItemID")
            else:
                print("ğŸ“ æ•°æ®åº“ä¸ºç©ºï¼Œå°†å¼€å§‹å…¨æ–°çˆ¬å–")
                
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®åº“æ•°æ®å¤±è´¥: {e}")
            self.scraped_itemids = set()

    def load_shop_list(self):
        """åŠ è½½åº—é“ºIDåˆ—è¡¨"""
        shops = []
        
        if not os.path.exists(self.shop_list_file):
            print(f"âŒ åº—é“ºåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {self.shop_list_file}")
            return shops
        
        with open(self.shop_list_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    try:
                        shop_id = line.strip()
                        shops.append(shop_id)
                    except ValueError:
                        continue
        
        print(f"ğŸ“‹ åŠ è½½äº† {len(shops)} ä¸ªåº—é“ºID")
        return shops

    async def connect_to_chrome(self):
        """è¿æ¥åˆ°ç°æœ‰çš„Chromeè°ƒè¯•ä¼šè¯"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get("http://localhost:9222/json/version") as response:
                    if response.status == 200:
                        version_info = await response.json()
                        ws_url = version_info.get('webSocketDebuggerUrl', '')
                        print("âœ… Chromeè°ƒè¯•ç«¯å£è¿æ¥æˆåŠŸ")
                    else:
                        print("âŒ Chromeè°ƒè¯•ç«¯å£æ— å“åº”")
                        return False
        except Exception as e:
            print(f"âŒ è¿æ¥Chromeè°ƒè¯•ç«¯å£å¤±è´¥: {e}")
            return False
        
        try:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.connect_over_cdp(ws_url)
            
            # ä½¿ç”¨ç°æœ‰æµè§ˆå™¨ä¸Šä¸‹æ–‡
            contexts = self.browser.contexts
            if contexts:
                context = contexts[0]
                print("ğŸ”„ ä½¿ç”¨ç°æœ‰æµè§ˆå™¨ä¸Šä¸‹æ–‡")
            else:
                context = await self.browser.new_context()
                print("ğŸ†• åˆ›å»ºæ–°çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡")
            
            # ä½¿ç”¨ç°æœ‰é¡µé¢æˆ–åˆ›å»ºæ–°é¡µé¢
            pages = context.pages
            if pages:
                self.page = pages[0]
                print("ğŸ”„ ä½¿ç”¨ç°æœ‰é¡µé¢")
            else:
                self.page = await context.new_page()
                print("ğŸ†• åˆ›å»ºæ–°é¡µé¢")
            
            return True
            
        except Exception as e:
            print(f"âŒ Playwrightè¿æ¥å¤±è´¥: {e}")
            return False

    async def extract_page_data(self, shop_id: str, page_num: int):
        """æå–å½“å‰é¡µé¢çš„ä¹¦ç±æ•°æ®"""
        try:
            books = await self.page.evaluate("""
                () => {
                    const books = [];
                    const bookItems = document.querySelectorAll('.item-info, .shopLineBookItem');
                    
                    bookItems.forEach(item => {
                        try {
                            const book = {};
                            
                            // ä¹¦ç±é“¾æ¥å’Œitemid
                            const linkElement = item.querySelector('a[href*="book.kongfz.com"]');
                            if (linkElement) {
                                book.book_url = linkElement.href;
                                const urlMatch = book.book_url.match(/\\/(\\d+)\\//);
                                if (urlMatch) {
                                    book.itemid = urlMatch[1];
                                }
                            }
                            
                            // ä¹¦å
                            const titleElement = item.querySelector('.title, .title-link, h3 a, .bookTitle');
                            if (titleElement) {
                                book.title = titleElement.textContent.trim();
                            }
                            
                            // ä½œè€…
                            const authorElement = item.querySelector('.author, .bookAuthor');
                            if (authorElement) {
                                book.author = authorElement.textContent.trim();
                            }
                            
                            // å‡ºç‰ˆç¤¾å’Œå‡ºç‰ˆå¹´ä»½
                            const publishElement = item.querySelector('.publish, .bookPub, .publisher');
                            if (publishElement) {
                                const publishText = publishElement.textContent.trim();
                                book.publisher = publishText.split(' ')[0] || '';
                                const yearMatch = publishText.match(/(\\d{4})/);
                                if (yearMatch) {
                                    book.publish_year = yearMatch[1];
                                }
                            }
                            
                            // ISBN
                            const isbnElement = item.querySelector('.isbn, [class*="isbn"]');
                            if (isbnElement) {
                                book.isbn = isbnElement.textContent.replace(/[^\\d]/g, '');
                            }
                            
                            // ä»·æ ¼
                            const priceElement = item.querySelector('.price, .bookPrice, .itemPrice');
                            if (priceElement) {
                                const priceText = priceElement.textContent.trim();
                                const priceMatch = priceText.match(/(\\d+\\.?\\d*)/);
                                if (priceMatch) {
                                    book.price = priceMatch[1];
                                    book.display_price = priceText;
                                }
                            }
                            
                            // å“ç›¸
                            const qualityElement = item.querySelector('.quality, .bookQuality');
                            if (qualityElement) {
                                book.quality = qualityElement.textContent.trim();
                            }
                            
                            // å…¶ä»–ä¿¡æ¯
                            const catnumElement = item.querySelector('.catnum');
                            if (catnumElement) {
                                book.catnum = catnumElement.textContent.trim();
                            }
                            
                            const useridElement = item.querySelector('[data-userid]');
                            if (useridElement) {
                                book.userid = useridElement.getAttribute('data-userid');
                            }
                            
                            if (book.itemid && book.title) {
                                books.push(book);
                            }
                            
                        } catch (error) {
                            console.log('æå–ä¹¦ç±ä¿¡æ¯æ—¶å‡ºé”™:', error);
                        }
                    });
                    
                    return books;
                }
            """)
            
            # è¡¥å……çˆ¬å–ä¿¡æ¯
            current_time = datetime.now().isoformat()
            for book in books:
                book['shopid'] = shop_id
                book['scraped_time'] = current_time
                book['scraped_shop_id'] = shop_id
                book['scraped_page'] = page_num
            
            return books
            
        except Exception as e:
            print(f"âŒ æå–é¡µé¢æ•°æ®å¤±è´¥: {e}")
            return []

    async def save_books_to_database(self, books_data):
        """ä¿å­˜ä¹¦ç±æ•°æ®åˆ°æ•°æ®åº“"""
        if not books_data:
            return 0
        
        try:
            saved_count = await self.book_repo.save_books(books_data)
            return saved_count
        except Exception as e:
            print(f"âŒ ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {e}")
            return 0

    async def scrape_shop(self, shop_id: str):
        """çˆ¬å–å•ä¸ªåº—é“ºçš„æ‰€æœ‰ä¹¦ç±"""
        print(f"\nğŸª å¼€å§‹çˆ¬å–åº—é“º {shop_id}")
        
        shop_url = f"https://shop{shop_id}.kongfz.com/book/"
        
        try:
            await self.page.goto(shop_url, wait_until='networkidle')
            await asyncio.sleep(3)
            
            page_num = 1
            max_pages = 1000  # è®¾ç½®æœ€å¤§é¡µæ•°é™åˆ¶
            consecutive_empty_pages = 0
            
            while page_num <= max_pages and consecutive_empty_pages < 3:
                print(f"ğŸ“š æ­£åœ¨çˆ¬å–ç¬¬ {page_num} é¡µ...")
                
                # æå–å½“å‰é¡µé¢æ•°æ®
                page_books = await self.extract_page_data(shop_id, page_num)
                
                if not page_books:
                    consecutive_empty_pages += 1
                    print(f"âš ï¸  ç¬¬ {page_num} é¡µæ— æ•°æ®ï¼Œè¿ç»­ç©ºé¡µ: {consecutive_empty_pages}")
                    if consecutive_empty_pages >= 3:
                        print("ğŸ”š è¿ç»­3é¡µæ— æ•°æ®ï¼Œç»“æŸå½“å‰åº—é“ºçˆ¬å–")
                        break
                else:
                    consecutive_empty_pages = 0
                
                # å»é‡å¤„ç†
                new_books = []
                duplicate_count = 0
                
                for book in page_books:
                    itemid = book.get('itemid', '')
                    if itemid and itemid not in self.scraped_itemids:
                        new_books.append(book)
                        self.scraped_itemids.add(itemid)
                    else:
                        duplicate_count += 1
                
                # ä¿å­˜æ–°ä¹¦ç±åˆ°æ•°æ®åº“
                if new_books:
                    saved_count = await self.save_books_to_database(new_books)
                    self.stats['new_records'] += saved_count
                    print(f"ğŸ“š ç¬¬ {page_num} é¡µ: {len(page_books)} æœ¬ä¹¦ï¼Œæ–°å¢ {len(new_books)} æœ¬")
                    print(f"ğŸ’¾ æˆåŠŸä¿å­˜ {saved_count} æ¡è®°å½•åˆ°æ•°æ®åº“")
                else:
                    print(f"ğŸ“š ç¬¬ {page_num} é¡µ: {len(page_books)} æœ¬ä¹¦ï¼Œæ–°å¢ 0 æœ¬")
                
                if duplicate_count > 0:
                    self.stats['duplicate_skipped'] += duplicate_count
                    print(f"ğŸ”„ è·³è¿‡ {duplicate_count} æ¡é‡å¤è®°å½•")
                
                self.stats['pages_processed'] += 1
                
                # å¦‚æœæ•´é¡µéƒ½æ˜¯é‡å¤æ•°æ®ï¼Œå¯èƒ½å·²ç»çˆ¬å®Œäº†
                if duplicate_count == len(page_books) and len(page_books) > 0:
                    print("âœ… å½“å‰é¡µå…¨éƒ¨ä¸ºé‡å¤æ•°æ®ï¼Œå¯èƒ½å·²çˆ¬å–å®Œæˆ")
                    break
                
                # å°è¯•ç¿»åˆ°ä¸‹ä¸€é¡µ
                if not await self.go_to_next_page():
                    print("ğŸ“„ æ— æ³•ç¿»åˆ°ä¸‹ä¸€é¡µï¼Œç»“æŸå½“å‰åº—é“º")
                    break
                
                page_num += 1
                await asyncio.sleep(2)  # é¡µé¢é—´éš”
            
            self.stats['shops_processed'] += 1
            print(f"âœ… åº—é“º {shop_id} çˆ¬å–å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ çˆ¬å–åº—é“º {shop_id} å¤±è´¥: {e}")

    async def go_to_next_page(self):
        """å°è¯•ç¿»åˆ°ä¸‹ä¸€é¡µ"""
        try:
            # å¸¸è§çš„ä¸‹ä¸€é¡µé€‰æ‹©å™¨
            next_selectors = [
                'a.next:not(.disabled)',
                'a[title="ä¸‹ä¸€é¡µ"]:not(.disabled)',
                '.pagination a.next',
                'a:has-text("ä¸‹ä¸€é¡µ")',
                '.page-next:not(.disabled)'
            ]
            
            for selector in next_selectors:
                try:
                    next_btn = await self.page.query_selector(selector)
                    if next_btn:
                        await next_btn.click()
                        await self.page.wait_for_load_state('networkidle')
                        await asyncio.sleep(1)
                        return True
                except:
                    continue
            
            return False
            
        except Exception as e:
            print(f"ç¿»é¡µå¤±è´¥: {e}")
            return False

    def print_final_stats(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "="*50)
        print("ğŸ“Š çˆ¬å–å®Œæˆç»Ÿè®¡")
        print("="*50)
        print(f"ğŸª å¤„ç†åº—é“ºæ•°: {self.stats['shops_processed']}")
        print(f"ğŸ“„ å¤„ç†é¡µé¢æ•°: {self.stats['pages_processed']}")
        print(f"ğŸ“š åŸæœ‰è®°å½•æ•°: {self.stats['existing_records']}")
        print(f"ğŸ†• æ–°å¢è®°å½•æ•°: {self.stats['new_records']}")
        print(f"ğŸ”„ è·³è¿‡é‡å¤æ•°: {self.stats['duplicate_skipped']}")
        print(f"ğŸ“Š æ€»è®°å½•æ•°: {self.stats['existing_records'] + self.stats['new_records']}")
        print("="*50)

    async def run(self):
        """è¿è¡Œå¢é‡çˆ¬è™«"""
        print("ğŸš€ å¯åŠ¨å¢é‡å¼å¤šåº—é“ºçˆ¬è™« (æ•°æ®åº“ç‰ˆ)")
        
        # 1. åŠ è½½å·²æœ‰æ•°æ®
        await self.load_existing_data()
        
        # 2. åŠ è½½åº—é“ºåˆ—è¡¨
        shop_list = self.load_shop_list()
        if not shop_list:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„åº—é“ºIDï¼Œé€€å‡º")
            return
        
        # 3. è¿æ¥åˆ°Chrome
        if not await self.connect_to_chrome():
            print("âŒ æ— æ³•è¿æ¥åˆ°Chromeï¼Œè¯·å…ˆå¯åŠ¨Chromeè°ƒè¯•æ¨¡å¼")
            return
        
        try:
            # 4. é€ä¸ªåº—é“ºçˆ¬å–
            for i, shop_id in enumerate(shop_list, 1):
                print(f"\n{'='*20} åº—é“º {i}/{len(shop_list)} {'='*20}")
                await self.scrape_shop(shop_id)
                
                # åº—é“ºé—´ç­‰å¾…
                if i < len(shop_list):
                    print("â³ åº—é“ºé—´ç­‰å¾… 5 ç§’...")
                    await asyncio.sleep(5)
            
            # 5. æ‰“å°æœ€ç»ˆç»Ÿè®¡
            self.print_final_stats()
            
        except KeyboardInterrupt:
            print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­çˆ¬å–")
            self.print_final_stats()
        except Exception as e:
            print(f"\nâŒ çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}")
            self.print_final_stats()
        finally:
            await self.cleanup()

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.playwright:
                await self.playwright.stop()
                print("ğŸ§¹ æµè§ˆå™¨è¿æ¥å·²å…³é—­")
        except:
            pass

async def main():
    scraper = IncrementalScraperV2()
    await scraper.run()

if __name__ == "__main__":
    asyncio.run(main())