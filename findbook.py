import asyncio
import json
import csv
from playwright.async_api import async_playwright
import time
import random
from urllib.parse import urljoin
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class KongfzShopScraper:
    def __init__(self, headless=False, slow_mo=1000):
        self.headless = headless
        self.slow_mo = slow_mo
        self.results = []
        self.browser = None
        self.context = None

    async def pre_login_setup(self):
        """é¢„å¤„ç†è®¾ç½® - è®©ç”¨æˆ·ç™»å½•"""
        async with async_playwright() as p:
            # å¯åŠ¨æµè§ˆå™¨
            self.browser = await p.chromium.launch(
                headless=self.headless,
                slow_mo=self.slow_mo
            )

            # åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡
            self.context = await self.browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )

            # åˆ›å»ºç™»å½•é¡µé¢
            login_page = await self.context.new_page()

            # æ‰“å¼€å­”å¤«å­æ—§ä¹¦ç½‘ç™»å½•é¡µ
            await login_page.goto('https://www.kongfz.com/user/login/', wait_until='networkidle')

            print("\n" + "="*60)
            print("ğŸ” è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆä»¥ä¸‹æ“ä½œ:")
            print("1. ç™»å½•æ‚¨çš„å­”å¤«å­æ—§ä¹¦ç½‘è´¦å·")
            print("2. ç¡®ä¿ç™»å½•æˆåŠŸ")
            print("3. åœ¨æ§åˆ¶å°è¾“å…¥ 'ok' å¹¶æŒ‰å›è½¦å¼€å§‹çˆ¬å–")
            print("4. æˆ–è€…è¾“å…¥ 'quit' é€€å‡ºç¨‹åº")
            print("="*60)

            # ç­‰å¾…ç”¨æˆ·ç¡®è®¤
            while True:
                user_input = input("\nè¯·è¾“å…¥æŒ‡ä»¤ (ok/quit): ").strip().lower()

                if user_input == 'ok':
                    logger.info("ç”¨æˆ·ç¡®è®¤å¼€å§‹çˆ¬å–")
                    await login_page.close()
                    return True
                elif user_input == 'quit':
                    logger.info("ç”¨æˆ·é€‰æ‹©é€€å‡º")
                    await login_page.close()
                    await self.context.close()
                    await self.browser.close()
                    return False
                else:
                    print("âŒ æ— æ•ˆè¾“å…¥ï¼Œè¯·è¾“å…¥ 'ok' å¼€å§‹çˆ¬å–æˆ– 'quit' é€€å‡º")

    async def scrape_shops(self, shop_urls):
        """çˆ¬å–å¤šä¸ªåº—é“º - ä½¿ç”¨å·²å»ºç«‹çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡"""
        try:
            for shop_url in shop_urls:
                logger.info(f"å¼€å§‹çˆ¬å–åº—é“º: {shop_url}")
                await self.scrape_single_shop(self.context, shop_url)

                # éšæœºå»¶è¿Ÿï¼Œé¿å…è¿‡å¿«è¯·æ±‚
                await asyncio.sleep(random.uniform(2, 5))

        except Exception as e:
            logger.error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        finally:
            if self.context:
                await self.context.close()
            if self.browser:
                await self.browser.close()

        return self.results

    async def scrape_single_shop(self, context, shop_url):
        """çˆ¬å–å•ä¸ªåº—é“º"""
        page = await context.new_page()

        try:
            # è®¿é—®åº—é“ºé¡µé¢
            logger.info(f"è®¿é—®åº—é“ºé¡µé¢: {shop_url}")
            await page.goto(shop_url, wait_until='networkidle')
            await asyncio.sleep(2)

            # ç‚¹å‡»"æŸ¥çœ‹å…¨éƒ¨"æŒ‰é’®
            view_all_selector = "body > div.main-box > div:nth-child(16) > div.content.clearfix > div.content-main > div:nth-child(8) > div > div.more_goods > a"

            try:
                await page.wait_for_selector(view_all_selector, timeout=10000)
                await page.click(view_all_selector)
                logger.info("å·²ç‚¹å‡»æŸ¥çœ‹å…¨éƒ¨æŒ‰é’®")
                await asyncio.sleep(3)

                # ç­‰å¾…åˆ—è¡¨é¡µé¢åŠ è½½
                await page.wait_for_load_state('networkidle')

                # å¼€å§‹çˆ¬å–åˆ†é¡µæ•°æ®
                await self.scrape_paginated_books(page, shop_url)

            except Exception as e:
                logger.error(f"ç‚¹å‡»æŸ¥çœ‹å…¨éƒ¨æŒ‰é’®å¤±è´¥: {e}")
                # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                alternative_selectors = [
                    "a[href*='itemlist']",
                    ".more_goods a",
                    "a:has-text('æŸ¥çœ‹å…¨éƒ¨')",
                    "a:has-text('æ›´å¤š')"
                ]

                for selector in alternative_selectors:
                    try:
                        await page.click(selector)
                        logger.info(f"ä½¿ç”¨å¤‡ç”¨é€‰æ‹©å™¨ {selector} æˆåŠŸ")
                        await asyncio.sleep(3)
                        await self.scrape_paginated_books(page, shop_url)
                        break
                    except:
                        continue
                else:
                    logger.error("æ‰€æœ‰é€‰æ‹©å™¨éƒ½å¤±è´¥äº†")

        except Exception as e:
            logger.error(f"çˆ¬å–åº—é“º {shop_url} æ—¶å‡ºé”™: {e}")
        finally:
            await page.close()

    async def scrape_paginated_books(self, page, shop_url):
        """çˆ¬å–åˆ†é¡µçš„ä¹¦ç±åˆ—è¡¨"""
        current_page = 1
        max_pages = 50  # è®¾ç½®æœ€å¤§é¡µæ•°é™åˆ¶ï¼Œé¿å…æ— é™å¾ªç¯

        while current_page <= max_pages:
            logger.info(f"æ­£åœ¨çˆ¬å–ç¬¬ {current_page} é¡µ")

            try:
                # ç­‰å¾…ä¹¦ç±åˆ—è¡¨åŠ è½½
                await page.wait_for_selector("#listBox", timeout=15000)
                await asyncio.sleep(2)

                # è·å–å½“å‰é¡µé¢çš„æ‰€æœ‰ä¹¦ç±é“¾æ¥
                book_links = await self.get_book_links(page)

                if not book_links:
                    logger.info("æ²¡æœ‰æ‰¾åˆ°æ›´å¤šä¹¦ç±ï¼Œç»“æŸçˆ¬å–")
                    break

                logger.info(f"ç¬¬ {current_page} é¡µæ‰¾åˆ° {len(book_links)} æœ¬ä¹¦ç±")

                # çˆ¬å–æ¯æœ¬ä¹¦çš„è¯¦æƒ…
                for i, book_link in enumerate(book_links):
                    logger.info(f"æ­£åœ¨çˆ¬å–ç¬¬ {current_page} é¡µç¬¬ {i+1} æœ¬ä¹¦: {book_link}")
                    await self.scrape_book_detail(page.context, book_link, shop_url)

                    # éšæœºå»¶è¿Ÿ
                    await asyncio.sleep(random.uniform(1, 3))

                # å°è¯•ç¿»åˆ°ä¸‹ä¸€é¡µ
                if not await self.go_to_next_page(page):
                    logger.info("æ²¡æœ‰ä¸‹ä¸€é¡µï¼Œç»“æŸçˆ¬å–")
                    break

                current_page += 1
                await asyncio.sleep(random.uniform(2, 4))

            except Exception as e:
                logger.error(f"çˆ¬å–ç¬¬ {current_page} é¡µæ—¶å‡ºé”™: {e}")
                break

    async def get_book_links(self, page):
        """è·å–å½“å‰é¡µé¢çš„æ‰€æœ‰ä¹¦ç±é“¾æ¥"""
        try:
            # ç­‰å¾…åˆ—è¡¨å®¹å™¨åŠ è½½
            await page.wait_for_selector("#listBox", timeout=10000)

            # è·å–æ‰€æœ‰ä¹¦ç±é“¾æ¥
            book_elements = await page.query_selector_all("#listBox .item-info .title a")

            book_links = []
            for element in book_elements:
                href = await element.get_attribute('href')
                if href:
                    # è½¬æ¢ä¸ºç»å¯¹URL
                    absolute_url = urljoin(page.url, href)
                    book_links.append(absolute_url)

            return book_links

        except Exception as e:
            logger.error(f"è·å–ä¹¦ç±é“¾æ¥å¤±è´¥: {e}")
            return []

    async def scrape_book_detail(self, context, book_url, shop_url):
        """çˆ¬å–ä¹¦ç±è¯¦æƒ…é¡µ"""
        detail_page = await context.new_page()

        try:
            # è®¿é—®ä¹¦ç±è¯¦æƒ…é¡µ
            await detail_page.goto(book_url, wait_until='networkidle')
            await asyncio.sleep(2)

            # ç›®æ ‡é€‰æ‹©å™¨
            target_selector = "body > div.main-box > div.main.content > div.main-bot.clear-fix > div.right-block > ul > li.item-detail-page > div.major-info.clear-fix > div.major-info-main > div.major-info-text > div > ul.detail-list1 > li:nth-child(5)"

            book_info = {
                'shop_url': shop_url,
                'book_url': book_url,
                'scraped_at': time.strftime('%Y-%m-%d %H:%M:%S')
            }

            try:
                # ç­‰å¾…ç›®æ ‡å…ƒç´ åŠ è½½
                await detail_page.wait_for_selector(target_selector, timeout=10000)
                target_element = await detail_page.query_selector(target_selector)

                if target_element:
                    target_text = await target_element.inner_text()
                    book_info['target_info'] = target_text.strip()
                    logger.info(f"æˆåŠŸæå–ç›®æ ‡ä¿¡æ¯: {target_text.strip()}")
                else:
                    book_info['target_info'] = "æœªæ‰¾åˆ°ç›®æ ‡å…ƒç´ "
                    logger.warning("æœªæ‰¾åˆ°ç›®æ ‡å…ƒç´ ")

            except Exception as e:
                book_info['target_info'] = f"æå–å¤±è´¥: {str(e)}"
                logger.error(f"æå–ç›®æ ‡ä¿¡æ¯å¤±è´¥: {e}")

            # å°è¯•æå–å…¶ä»–æœ‰ç”¨ä¿¡æ¯
            try:
                # ä¹¦å
                title_selectors = [
                    "h1.title",
                    ".book-title",
                    ".item-title",
                    "h1"
                ]

                for selector in title_selectors:
                    try:
                        title_element = await detail_page.query_selector(selector)
                        if title_element:
                            book_info['title'] = await title_element.inner_text()
                            break
                    except:
                        continue

                # ä»·æ ¼
                price_selectors = [
                    ".price",
                    ".book-price",
                    "[class*='price']"
                ]

                for selector in price_selectors:
                    try:
                        price_element = await detail_page.query_selector(selector)
                        if price_element:
                            book_info['price'] = await price_element.inner_text()
                            break
                    except:
                        continue

                # ä½œè€…
                author_selectors = [
                    ".author",
                    "[class*='author']",
                    "ul.detail-list1 li:nth-child(1)",
                    "ul.detail-list1 li:nth-child(2)"
                ]

                for selector in author_selectors:
                    try:
                        author_element = await detail_page.query_selector(selector)
                        if author_element:
                            author_text = await author_element.inner_text()
                            if 'ä½œè€…' in author_text or 'è‘—' in author_text:
                                book_info['author'] = author_text.strip()
                                break
                    except:
                        continue

            except Exception as e:
                logger.error(f"æå–é¢å¤–ä¿¡æ¯å¤±è´¥: {e}")

            # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
            self.results.append(book_info)
            logger.info(f"æˆåŠŸçˆ¬å–ä¹¦ç±ä¿¡æ¯: {book_info.get('title', 'æœªçŸ¥ä¹¦å')}")

        except Exception as e:
            logger.error(f"çˆ¬å–ä¹¦ç±è¯¦æƒ… {book_url} å¤±è´¥: {e}")
        finally:
            await detail_page.close()

    async def go_to_next_page(self, page):
        """ç¿»åˆ°ä¸‹ä¸€é¡µ"""
        try:
            # å¯»æ‰¾ä¸‹ä¸€é¡µæŒ‰é’®
            next_page_selectors = [
                "a:has-text('ä¸‹ä¸€é¡µ')",
                ".next",
                "[class*='next']",
                "a[href*='page']",
                ".pagination a:last-child"
            ]

            for selector in next_page_selectors:
                try:
                    next_button = await page.query_selector(selector)
                    if next_button:
                        # æ£€æŸ¥æŒ‰é’®æ˜¯å¦å¯ç‚¹å‡»
                        is_disabled = await next_button.get_attribute('disabled')
                        class_name = await next_button.get_attribute('class') or ''

                        if not is_disabled and 'disabled' not in class_name:
                            await next_button.click()
                            logger.info("æˆåŠŸç¿»åˆ°ä¸‹ä¸€é¡µ")
                            await page.wait_for_load_state('networkidle')
                            return True
                except Exception as e:
                    logger.debug(f"é€‰æ‹©å™¨ {selector} å¤±è´¥: {e}")
                    continue

            logger.info("æ²¡æœ‰æ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®æˆ–å·²åˆ°æœ€åä¸€é¡µ")
            return False

        except Exception as e:
            logger.error(f"ç¿»é¡µå¤±è´¥: {e}")
            return False

    def save_results(self, filename_prefix="kongfz_books"):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        if not self.results:
            logger.warning("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return

        timestamp = time.strftime('%Y%m%d_%H%M%S')

        # ä¿å­˜ä¸ºJSON
        json_filename = f"{filename_prefix}_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ° {json_filename}")

        # ä¿å­˜ä¸ºCSV
        csv_filename = f"{filename_prefix}_{timestamp}.csv"
        if self.results:
            fieldnames = self.results[0].keys()
            with open(csv_filename, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(self.results)
            logger.info(f"ç»“æœå·²ä¿å­˜åˆ° {csv_filename}")

        logger.info(f"æ€»å…±çˆ¬å–äº† {len(self.results)} æœ¬ä¹¦ç±çš„ä¿¡æ¯")

async def main():
    """ä¸»å‡½æ•°"""
    # åº—é“ºURLåˆ—è¡¨ - ç”¨æˆ·è¾“å…¥
    shop_urls = [
        "https://shop.kongfz.com/534779",
        "https://shop.kongfz.com/726495",
        "https://shop.kongfz.com/269228"
    ]

    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    scraper = KongfzShopScraper(
        headless=False,  # æœ‰å¤´æ¨¡å¼ï¼Œç”¨æˆ·å¯ä»¥çœ‹åˆ°æµè§ˆå™¨
        slow_mo=1000     # æ”¾æ…¢æ“ä½œé€Ÿåº¦ï¼Œé¿å…è¢«æ£€æµ‹
    )

    try:
        print("ğŸš€ å¯åŠ¨å­”å¤«å­æ—§ä¹¦ç½‘çˆ¬è™«...")

        # é¢„å¤„ç† - ç”¨æˆ·ç™»å½•
        if not await scraper.pre_login_setup():
            print("âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œï¼Œç¨‹åºé€€å‡º")
            return

        # å¼€å§‹çˆ¬å–
        logger.info("å¼€å§‹çˆ¬å–å­”å¤«å­æ—§ä¹¦ç½‘åº—é“º...")
        results = await scraper.scrape_shops(shop_urls)

        # ä¿å­˜ç»“æœ
        scraper.save_results()

        print("\n" + "="*60)
        print(f"ğŸ‰ çˆ¬å–å®Œæˆ! æ€»å…±è·å–äº† {len(scraper.results)} æœ¬ä¹¦ç±ä¿¡æ¯")
        print("ğŸ“ æ•°æ®å·²ä¿å­˜ä¸º JSON å’Œ CSV æ ¼å¼")
        print("="*60)

    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")

# æ·»åŠ ä¸€ä¸ªç‹¬ç«‹çš„æµ‹è¯•ç™»å½•åŠŸèƒ½
async def test_login_only():
    """ä»…æµ‹è¯•ç™»å½•åŠŸèƒ½"""
    scraper = KongfzShopScraper(headless=False, slow_mo=1000)

    print("ğŸ§ª æµ‹è¯•ç™»å½•åŠŸèƒ½...")
    success = await scraper.pre_login_setup()

    if success:
        print("âœ… ç™»å½•æµ‹è¯•æˆåŠŸ!")
    else:
        print("âŒ ç™»å½•æµ‹è¯•å¤±è´¥æˆ–ç”¨æˆ·å–æ¶ˆ")

# æ·»åŠ ä¸€ä¸ªå¿«é€Ÿçˆ¬å–åŠŸèƒ½ï¼ˆè·³è¿‡ç™»å½•ï¼‰
async def quick_scrape():
    """å¿«é€Ÿçˆ¬å–ï¼ˆè·³è¿‡ç™»å½•æ­¥éª¤ï¼‰"""
    shop_urls = [
        "https://shop.kongfz.com/534779",
        "https://shop.kongfz.com/726495",
        "https://shop.kongfz.com/269228"
    ]

    scraper = KongfzShopScraper(headless=False, slow_mo=1000)

    # ç›´æ¥å¯åŠ¨æµè§ˆå™¨
    async with async_playwright() as p:
        scraper.browser = await p.chromium.launch(headless=False, slow_mo=1000)
        scraper.context = await scraper.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        )

        print("ğŸš€ å¿«é€Ÿæ¨¡å¼å¯åŠ¨ï¼Œè·³è¿‡ç™»å½•ç›´æ¥çˆ¬å–...")
        results = await scraper.scrape_shops(shop_urls)
        scraper.save_results()
        print(f"ğŸ‰ å¿«é€Ÿçˆ¬å–å®Œæˆ! è·å–äº† {len(results)} æœ¬ä¹¦ç±ä¿¡æ¯")

if __name__ == "__main__":
    import sys

    print("ğŸŒŸ å­”å¤«å­æ—§ä¹¦ç½‘çˆ¬è™«ç¨‹åº")
    print("è¯·é€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("1. å®Œæ•´æ¨¡å¼ (åŒ…å«ç™»å½•æ­¥éª¤)")
    print("2. å¿«é€Ÿæ¨¡å¼ (è·³è¿‡ç™»å½•)")
    print("3. ä»…æµ‹è¯•ç™»å½•")

    choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1/2/3): ").strip()

    if choice == "1":
        print("ğŸ” å¯åŠ¨å®Œæ•´æ¨¡å¼...")
        asyncio.run(main())
    elif choice == "2":
        print("âš¡ å¯åŠ¨å¿«é€Ÿæ¨¡å¼...")
        asyncio.run(quick_scrape())
    elif choice == "3":
        print("ğŸ§ª å¯åŠ¨ç™»å½•æµ‹è¯•...")
        asyncio.run(test_login_only())
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤å®Œæ•´æ¨¡å¼")
        asyncio.run(main())
