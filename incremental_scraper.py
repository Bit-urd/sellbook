#!/usr/bin/env python3
"""
å¢é‡å¼å¤šåº—é“ºçˆ¬è™« - æ”¯æŒæ–­ç‚¹ç»­çˆ¬å’Œå»é‡
- æ¯é¡µæ•°æ®ç«‹å³ä¿å­˜åˆ°CSV
- å¯åŠ¨æ—¶åŠ è½½å·²æœ‰æ•°æ®ï¼Œè‡ªåŠ¨å»é‡
- å·²çˆ¬å–çš„ä¹¦ç±ä¸å†é‡å¤çˆ¬å–
"""
import asyncio
import csv
import os
import time
from datetime import datetime
from playwright.async_api import async_playwright
import aiohttp

class IncrementalScraper:
    def __init__(self, shop_list_file="shop_list.txt", data_file="books_data.csv"):
        self.shop_list_file = shop_list_file
        self.data_file = data_file
        self.browser = None
        self.page = None
        self.playwright = None
        
        # å·²çˆ¬å–çš„ä¹¦ç±é›†åˆ(ç”¨äºå¿«é€Ÿå»é‡)
        self.scraped_itemids = set()
        self.scraped_count = 0
        
        # CSVå­—æ®µå®šä¹‰
        self.fieldnames = [
            'itemid', 'shopid', 'isbn', 'title', 'author', 'publisher', 
            'publish_year', 'quality', 'price', 'display_price', 
            'book_url', 'catnum', 'userid', 'scraped_time', 'scraped_shop_id', 'scraped_page'
        ]
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'existing_records': 0,
            'new_records': 0,
            'duplicate_skipped': 0,
            'shops_processed': 0,
            'pages_processed': 0
        }

    def load_existing_data(self):
        """åŠ è½½å·²æœ‰çš„CSVæ•°æ®ï¼Œæ„å»ºå»é‡é›†åˆ"""
        print("ğŸ“‚ æ£€æŸ¥å·²æœ‰æ•°æ®æ–‡ä»¶...")
        
        if not os.path.exists(self.data_file):
            print(f"ğŸ“ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶: {self.data_file}")
            self.create_csv_file()
            return
        
        try:
            with open(self.data_file, 'r', encoding='utf-8-sig') as csvfile:
                reader = csv.DictReader(csvfile)
                
                for row in reader:
                    itemid = row.get('itemid', '').strip()
                    if itemid:
                        self.scraped_itemids.add(itemid)
                        self.scraped_count += 1
            
            self.stats['existing_records'] = self.scraped_count
            print(f"ğŸ“Š åŠ è½½å·²æœ‰æ•°æ®: {self.scraped_count} æ¡è®°å½•")
            print(f"ğŸ” å»é‡é›†åˆå¤§å°: {len(self.scraped_itemids)} ä¸ªItemID")
            
        except Exception as e:
            print(f"âŒ åŠ è½½å·²æœ‰æ•°æ®å¤±è´¥: {e}")
            print("ğŸ”„ å°†åˆ›å»ºæ–°çš„æ•°æ®æ–‡ä»¶")
            self.create_csv_file()

    def create_csv_file(self):
        """åˆ›å»ºæ–°çš„CSVæ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´"""
        try:
            with open(self.data_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=self.fieldnames)
                writer.writeheader()
            print(f"âœ… åˆ›å»ºæ–°æ•°æ®æ–‡ä»¶: {self.data_file}")
        except Exception as e:
            print(f"âŒ åˆ›å»ºCSVæ–‡ä»¶å¤±è´¥: {e}")

    def append_to_csv(self, books_data):
        """è¿½åŠ æ–°æ•°æ®åˆ°CSVæ–‡ä»¶"""
        if not books_data:
            return 0
        
        new_records = 0
        duplicates = 0
        
        try:
            with open(self.data_file, 'a', newline='', encoding='utf-8-sig') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=self.fieldnames)
                
                for book in books_data:
                    itemid = book.get('itemid', '').strip()
                    
                    # å»é‡æ£€æŸ¥
                    if itemid in self.scraped_itemids:
                        duplicates += 1
                        continue
                    
                    # æ·»åŠ æ—¶é—´æˆ³
                    book['scraped_time'] = datetime.now().isoformat()
                    
                    # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨
                    row = {field: book.get(field, '') for field in self.fieldnames}
                    writer.writerow(row)
                    
                    # æ›´æ–°å»é‡é›†åˆ
                    self.scraped_itemids.add(itemid)
                    new_records += 1
            
            self.stats['new_records'] += new_records
            self.stats['duplicate_skipped'] += duplicates
            
            if new_records > 0:
                print(f"ğŸ’¾ æ–°å¢ {new_records} æ¡è®°å½•åˆ° {self.data_file}")
            if duplicates > 0:
                print(f"ğŸ”„ è·³è¿‡ {duplicates} æ¡é‡å¤è®°å½•")
            
            return new_records
            
        except Exception as e:
            print(f"âŒ è¿½åŠ æ•°æ®åˆ°CSVå¤±è´¥: {e}")
            return 0

    def load_shop_list(self):
        """ä»æ–‡ä»¶åŠ è½½åº—é“ºIDåˆ—è¡¨"""
        shop_ids = []
        
        if not os.path.exists(self.shop_list_file):
            print(f"âŒ åº—é“ºåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {self.shop_list_file}")
            return shop_ids
        
        try:
            with open(self.shop_list_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    
                    if line.isdigit():
                        shop_ids.append(line)
                    else:
                        print(f"âš ï¸ ç¬¬{line_num}è¡Œæ ¼å¼é”™è¯¯ï¼Œè·³è¿‡: {line}")
            
            print(f"ğŸ“‹ åŠ è½½äº† {len(shop_ids)} ä¸ªåº—é“ºID")
            return shop_ids
            
        except Exception as e:
            print(f"âŒ è¯»å–åº—é“ºåˆ—è¡¨å¤±è´¥: {e}")
            return shop_ids

    async def connect_to_chrome(self):
        """è¿æ¥åˆ°çœŸå®Chromeæµè§ˆå™¨"""
        print("ğŸ”— è¿æ¥çœŸå®Chromeæµè§ˆå™¨...")
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get("http://localhost:9222/json/version") as response:
                    if response.status == 200:
                        version_info = await response.json()
                        ws_url = version_info.get('webSocketDebuggerUrl', '')
                        print("âœ… æ£€æµ‹åˆ°Chromeå·²å¯åŠ¨")
                    else:
                        self.print_chrome_instructions()
                        return False
        except Exception as e:
            print(f"âŒ Chromeè¿æ¥å¤±è´¥: {e}")
            self.print_chrome_instructions()
            return False
        
        self.playwright = await async_playwright().start()
        
        try:
            self.browser = await self.playwright.chromium.connect_over_cdp(ws_url)
            context = await self.browser.new_context()
            self.page = await context.new_page()
            print("âœ… æˆåŠŸè¿æ¥åˆ°Chromeæµè§ˆå™¨")
            return True
            
        except Exception as e:
            print(f"âŒ è¿æ¥å¤±è´¥: {e}")
            if self.playwright:
                await self.playwright.stop()
            return False

    def print_chrome_instructions(self):
        """æ‰“å°Chromeå¯åŠ¨æŒ‡ä»¤"""
        print("\nâŒ Chromeæœªå¯åŠ¨ï¼Œè¯·å…ˆå¯åŠ¨Chromeè°ƒè¯•æ¨¡å¼:")
        print("macOS:")
        print("/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --remote-debugging-port=9222 --user-data-dir=/tmp/chrome-debug-session")

    async def scrape_single_shop(self, shop_id):
        """çˆ¬å–å•ä¸ªåº—é“ºçš„æ‰€æœ‰é¡µé¢ï¼Œå®æ—¶ä¿å­˜æ¯é¡µæ•°æ®"""
        shop_url = f"https://shop.kongfz.com/{shop_id}/all/"
        
        try:
            print(f"\nğŸª å¼€å§‹çˆ¬å–åº—é“º {shop_id}")
            print(f"ğŸ“ è®¿é—®: {shop_url}")
            
            await self.page.goto(shop_url, wait_until='networkidle')
            await asyncio.sleep(3)
            
            # æ£€æŸ¥é¡µé¢æ˜¯å¦æ­£ç¡®åŠ è½½
            page_title = await self.page.title()
            if "404" in page_title or "æ‰¾ä¸åˆ°" in page_title:
                print(f"âŒ åº—é“º {shop_id} ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®")
                return
            
            try:
                await self.page.wait_for_selector("#listBox", timeout=10000)
            except:
                print(f"âš ï¸ åº—é“º {shop_id} åˆ—è¡¨é¡µé¢åŠ è½½å¼‚å¸¸ï¼Œå°è¯•ç»§ç»­")
            
            current_page = 1
            consecutive_empty_pages = 0
            max_empty_pages = 3
            
            while True:
                print(f"ğŸ“„ çˆ¬å–åº—é“º {shop_id} ç¬¬ {current_page} é¡µ...")
                await asyncio.sleep(2)
                
                # æå–å½“å‰é¡µé¢æ•°æ®
                books_on_page = await self.extract_book_items()
                
                if not books_on_page:
                    consecutive_empty_pages += 1
                    print(f"ğŸ“‹ ç¬¬ {current_page} é¡µæ²¡æœ‰æ•°æ®")
                    
                    if consecutive_empty_pages >= max_empty_pages:
                        print(f"â¹ï¸ è¿ç»­{max_empty_pages}é¡µæ— æ•°æ®ï¼Œç»“æŸåº—é“º {shop_id}")
                        break
                else:
                    consecutive_empty_pages = 0
                    
                    # ä¸ºæ¯æœ¬ä¹¦æ·»åŠ çˆ¬å–ä¿¡æ¯
                    for book in books_on_page:
                        book['scraped_shop_id'] = shop_id
                        book['scraped_page'] = current_page
                    
                    # ç«‹å³ä¿å­˜åˆ°CSV
                    new_count = self.append_to_csv(books_on_page)
                    print(f"ğŸ“š ç¬¬ {current_page} é¡µ: {len(books_on_page)} æœ¬ä¹¦ï¼Œæ–°å¢ {new_count} æœ¬")
                    
                    self.stats['pages_processed'] += 1
                
                # å°è¯•ç¿»é¡µ
                if not await self.go_to_next_page():
                    print(f"ğŸ“„ åº—é“º {shop_id} å·²è¾¾åˆ°æœ€åä¸€é¡µ")
                    break
                
                current_page += 1
                
                if current_page > 1000:  # é˜²æ­¢æ— é™ç¿»é¡µ
                    print(f"âš ï¸ åº—é“º {shop_id} è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶")
                    break
                
                await asyncio.sleep(3)  # ç¿»é¡µå»¶è¿Ÿ
            
            self.stats['shops_processed'] += 1
            print(f"âœ… åº—é“º {shop_id} çˆ¬å–å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ çˆ¬å–åº—é“º {shop_id} æ—¶å‡ºé”™: {e}")

    async def extract_book_items(self):
        """æå–å½“å‰é¡µé¢çš„ä¹¦ç±ä¿¡æ¯"""
        try:
            books_data = await self.page.evaluate("""
                () => {
                    const books = [];
                    const itemRows = document.querySelectorAll('.item-row');
                    
                    itemRows.forEach(row => {
                        try {
                            const book = {
                                shopid: row.getAttribute('shopid') || '',
                                itemid: row.getAttribute('itemid') || '',
                                isbn: row.getAttribute('isbn') || '',
                                catnum: row.getAttribute('catnum') || '',
                                userid: row.getAttribute('userid') || '',
                                price: row.getAttribute('price') || '',
                                title: '',
                                book_url: '',
                                author: '',
                                publisher: '',
                                publish_year: '',
                                quality: '',
                                display_price: ''
                            };
                            
                            const nameLink = row.querySelector('.row-name');
                            if (nameLink) {
                                book.title = nameLink.textContent.trim();
                                book.book_url = nameLink.href;
                            }
                            
                            const author = row.querySelector('.row-author');
                            if (author) book.author = author.textContent.trim();
                            
                            const publisher = row.querySelector('.row-press');
                            if (publisher) book.publisher = publisher.textContent.trim();
                            
                            const year = row.querySelector('.row-years');
                            if (year) book.publish_year = year.textContent.trim();
                            
                            const quality = row.querySelector('.row-quality');
                            if (quality) book.quality = quality.textContent.trim();
                            
                            const priceElement = row.querySelector('.row-price .bold');
                            if (priceElement) book.display_price = priceElement.textContent.trim();
                            
                            books.push(book);
                            
                        } catch (error) {
                            console.log('æå–ä¹¦ç±ä¿¡æ¯æ—¶å‡ºé”™:', error);
                        }
                    });
                    
                    return books;
                }
            """)
            
            return books_data
            
        except Exception as e:
            print(f"âŒ æå–ä¹¦ç±ä¿¡æ¯å¤±è´¥: {e}")
            return []

    async def go_to_next_page(self):
        """ç¿»åˆ°ä¸‹ä¸€é¡µ"""
        try:
            next_btn = await self.page.query_selector("#pagerBox > a.next-btn")
            
            if next_btn:
                class_name = await next_btn.get_attribute('class') or ''
                if 'disabled' not in class_name.lower():
                    await next_btn.click()
                    await self.page.wait_for_load_state('networkidle')
                    return True
            
            return False
            
        except Exception as e:
            print(f"âŒ ç¿»é¡µå¤±è´¥: {e}")
            return False

    def print_final_stats(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "="*60)
        print("ğŸ“Š å¢é‡çˆ¬å–å®Œæˆç»Ÿè®¡")
        print("="*60)
        print(f"å·²æœ‰è®°å½•æ•°: {self.stats['existing_records']}")
        print(f"æ–°å¢è®°å½•æ•°: {self.stats['new_records']}")
        print(f"è·³è¿‡é‡å¤æ•°: {self.stats['duplicate_skipped']}")
        print(f"å¤„ç†åº—é“ºæ•°: {self.stats['shops_processed']}")
        print(f"å¤„ç†é¡µé¢æ•°: {self.stats['pages_processed']}")
        print(f"æ€»è®°å½•æ•°: {self.stats['existing_records'] + self.stats['new_records']}")
        print(f"æ•°æ®æ–‡ä»¶: {self.data_file}")
        print("="*60)

    async def run(self):
        """ä¸»è¿è¡Œæ–¹æ³•"""
        try:
            print("ğŸš€ å¯åŠ¨å¢é‡å¼å¤šåº—é“ºçˆ¬è™«...")
            
            # åŠ è½½å·²æœ‰æ•°æ®
            self.load_existing_data()
            
            # åŠ è½½åº—é“ºåˆ—è¡¨
            shop_ids = self.load_shop_list()
            if not shop_ids:
                print("âŒ æ²¡æœ‰å¯çˆ¬å–çš„åº—é“º")
                return
            
            # è¿æ¥Chrome
            if not await self.connect_to_chrome():
                return
            
            # é€ä¸ªçˆ¬å–åº—é“º
            for i, shop_id in enumerate(shop_ids, 1):
                print(f"\n{'='*20} è¿›åº¦: {i}/{len(shop_ids)} {'='*20}")
                
                try:
                    await self.scrape_single_shop(shop_id)
                    
                    # åº—é“ºé—´å»¶è¿Ÿ
                    if i < len(shop_ids):
                        print("â³ ç­‰å¾…5ç§’åç»§ç»­ä¸‹ä¸€ä¸ªåº—é“º...")
                        await asyncio.sleep(5)
                        
                except Exception as e:
                    print(f"âŒ åº—é“º {shop_id} çˆ¬å–å¤±è´¥: {e}")
            
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            self.print_final_stats()
            
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­çˆ¬å–")
            self.print_final_stats()
            
        except Exception as e:
            print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
            
        finally:
            if self.playwright:
                await self.playwright.stop()
            print("ğŸ”Œ å·²æ–­å¼€è¿æ¥")

async def main():
    """ç¨‹åºå…¥å£"""
    scraper = IncrementalScraper()
    await scraper.run()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹åºå·²é€€å‡º")
    except Exception as e:
        print(f"âŒ ç¨‹åºå¼‚å¸¸é€€å‡º: {e}")